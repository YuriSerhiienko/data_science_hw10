{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5lLL6lYe9z4WCAfdPGial",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuriSerhiienko/data_science_hw10/blob/main/hw10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo41pF7drehj",
        "outputId": "020753a5-93c7-4ccc-8d15-f0a56ef87616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 8s 4ms/step - loss: 0.4683 - accuracy: 0.8298 - val_loss: 0.3533 - val_accuracy: 0.8733\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.3108 - accuracy: 0.8876 - val_loss: 0.2913 - val_accuracy: 0.8939\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2645 - accuracy: 0.9035 - val_loss: 0.2891 - val_accuracy: 0.8908\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.2323 - accuracy: 0.9142 - val_loss: 0.2651 - val_accuracy: 0.9055\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2048 - accuracy: 0.9244 - val_loss: 0.2686 - val_accuracy: 0.9046\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1802 - accuracy: 0.9327 - val_loss: 0.2445 - val_accuracy: 0.9116\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1593 - accuracy: 0.9403 - val_loss: 0.2660 - val_accuracy: 0.9072\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1406 - accuracy: 0.9472 - val_loss: 0.3005 - val_accuracy: 0.9038\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1251 - accuracy: 0.9536 - val_loss: 0.2654 - val_accuracy: 0.9109\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1081 - accuracy: 0.9592 - val_loss: 0.2915 - val_accuracy: 0.9150\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3098 - accuracy: 0.9088\n",
            "\n",
            "Точність на тестових даних: 0.9088000059127808\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "\n",
            "Матриця помилок:\n",
            "[[860   0  19  10   6   1 100   0   3   1]\n",
            " [  2 980   0  12   3   0   2   0   1   0]\n",
            " [ 19   0 884  10  38   0  49   0   0   0]\n",
            " [ 10   2   7 940  19   0  20   0   2   0]\n",
            " [  3   1  64  26 860   0  43   0   3   0]\n",
            " [  0   0   0   1   0 946   0  36   2  15]\n",
            " [111   0  78  23  69   0 712   1   6   0]\n",
            " [  0   0   0   0   0   3   0 988   0   9]\n",
            " [  5   0   6   2   3   3   4   4 973   0]\n",
            " [  1   0   0   0   0   1   0  53   0 945]]\n",
            "\n",
            "Класифікаційний звіт:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86      1000\n",
            "           1       1.00      0.98      0.99      1000\n",
            "           2       0.84      0.88      0.86      1000\n",
            "           3       0.92      0.94      0.93      1000\n",
            "           4       0.86      0.86      0.86      1000\n",
            "           5       0.99      0.95      0.97      1000\n",
            "           6       0.77      0.71      0.74      1000\n",
            "           7       0.91      0.99      0.95      1000\n",
            "           8       0.98      0.97      0.98      1000\n",
            "           9       0.97      0.94      0.96      1000\n",
            "\n",
            "    accuracy                           0.91     10000\n",
            "   macro avg       0.91      0.91      0.91     10000\n",
            "weighted avg       0.91      0.91      0.91     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Нормалізація зображень\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Зміна розмірів зображень для CNN\n",
        "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
        "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=10, validation_split=0.2)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('\\nТочність на тестових даних:', test_acc)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"\\nМатриця помилок:\")\n",
        "print(confusion_matrix(test_labels, y_pred))\n",
        "\n",
        "print(\"\\nКласифікаційний звіт:\")\n",
        "print(classification_report(test_labels, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Завантаження та підготовка датасету\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Зміна розмірів зображень для VGG16\n",
        "def resize_images(images):\n",
        "    images_resized = np.array([tf.image.resize(img, (32, 32)) for img in images])\n",
        "    return images_resized\n",
        "\n",
        "train_images_resized = resize_images(train_images[..., np.newaxis])\n",
        "test_images_resized = resize_images(test_images[..., np.newaxis])\n",
        "\n",
        "# Конвертація зображень у формат RGB\n",
        "train_images_rgb = np.repeat(train_images_resized, 3, axis=-1)\n",
        "test_images_rgb = np.repeat(test_images_resized, 3, axis=-1)\n",
        "\n",
        "# Завантаження VGG16\n",
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "vgg_base.trainable = False  # Заморожування ваг\n",
        "\n",
        "# Створення моделі\n",
        "model = tf.keras.Sequential([\n",
        "    vgg_base,\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Навчання моделі\n",
        "history = model.fit(train_images_rgb, train_labels, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Оцінка моделі\n",
        "test_loss, test_acc = model.evaluate(test_images_rgb, test_labels)\n",
        "print('\\nТочність на тестових даних:', test_acc)\n",
        "\n",
        "# Передбачення та метрики якості\n",
        "predictions = model.predict(test_images_rgb)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"\\nМатриця помилок:\")\n",
        "print(confusion_matrix(test_labels, y_pred))\n",
        "\n",
        "print(\"\\nКласифікаційний звіт:\")\n",
        "print(classification_report(test_labels, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvFE4hPPvPLS",
        "outputId": "eb5eb55a-41fe-426a-e42f-4cceb2cee2dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 19s 10ms/step - loss: 0.6978 - accuracy: 0.7533 - val_loss: 0.4747 - val_accuracy: 0.8224\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.5068 - accuracy: 0.8180 - val_loss: 0.4394 - val_accuracy: 0.8384\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 18s 12ms/step - loss: 0.4708 - accuracy: 0.8303 - val_loss: 0.4269 - val_accuracy: 0.8422\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4462 - accuracy: 0.8393 - val_loss: 0.4096 - val_accuracy: 0.8477\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.4281 - accuracy: 0.8461 - val_loss: 0.4006 - val_accuracy: 0.8492\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4185 - accuracy: 0.8479 - val_loss: 0.3954 - val_accuracy: 0.8539\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.4061 - accuracy: 0.8520 - val_loss: 0.3788 - val_accuracy: 0.8623\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.3982 - accuracy: 0.8537 - val_loss: 0.3842 - val_accuracy: 0.8601\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.3911 - accuracy: 0.8549 - val_loss: 0.3853 - val_accuracy: 0.8549\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.3844 - accuracy: 0.8602 - val_loss: 0.3747 - val_accuracy: 0.8618\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3775 - accuracy: 0.8595\n",
            "\n",
            "Точність на тестових даних: 0.859499990940094\n",
            "313/313 [==============================] - 2s 7ms/step\n",
            "\n",
            "Матриця помилок:\n",
            "[[786   1  14  45   4   1 139   1   9   0]\n",
            " [  0 964   1  25   2   0   7   0   1   0]\n",
            " [  8   0 730  14 128   0 118   0   2   0]\n",
            " [ 25  11  13 845  48   0  55   0   3   0]\n",
            " [  4   1  71  40 793   0  90   0   1   0]\n",
            " [  0   0   0   0   0 950   0  36   0  14]\n",
            " [103   2  51  42 120   0 668   0  14   0]\n",
            " [  0   0   0   0   0  27   0 948   0  25]\n",
            " [  4   0   4   7   8   4   6   1 964   2]\n",
            " [  0   0   1   0   0  10   0  42   0 947]]\n",
            "\n",
            "Класифікаційний звіт:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.79      0.81      1000\n",
            "           1       0.98      0.96      0.97      1000\n",
            "           2       0.82      0.73      0.77      1000\n",
            "           3       0.83      0.84      0.84      1000\n",
            "           4       0.72      0.79      0.75      1000\n",
            "           5       0.96      0.95      0.95      1000\n",
            "           6       0.62      0.67      0.64      1000\n",
            "           7       0.92      0.95      0.93      1000\n",
            "           8       0.97      0.96      0.97      1000\n",
            "           9       0.96      0.95      0.95      1000\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.86      0.86      0.86     10000\n",
            "weighted avg       0.86      0.86      0.86     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попередня робота (Багатошаровий перцептрон)\n",
        "\n",
        "Архітектура: Базувалася на повнозв'язних шарах (Dense layers).\n",
        "\n",
        "Точність на тестових даних: 88.08% у базовій моделі, 86.93% після оновлення архітектури.\n",
        "\n",
        "Інші метрики: Висока точність (precision) та повнота (recall), особливо після оновлення архітектури.\n",
        "\n",
        "Нинішня робота (Згорткова нейронна мережа, CNN)\n",
        "\n",
        "Архітектура: Базувалася на згорткових шарах (Conv2D), що є типовим для задач обробки зображень.\n",
        "\n",
        "Точність на тестових даних: 90.88%.\n",
        "\n",
        "Інші метрики: Деталізовані метрики (матриця помилок, точність, повнота, F1-оцінка) показали високу ефективність моделі у класифікації зображень.\n",
        "\n",
        "Висновки\n",
        "\n",
        "Ефективність архітектури: Згорткова нейронна мережа (CNN) показала кращі результати для задачі класифікації зображень у порівнянні з багатошаровим перцептроном. Це очікувано, оскільки CNN краще адаптовані для виявлення шаблонів у зображеннях.\n",
        "\n",
        "Підхід до обробки даних: CNN використовує просторові відносини між пікселями, що дозволяє ефективно виділяти особливості зображень, в той час як багатошаровий перцептрон обробляє дані як одновимірний масив.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZxPkg8zZMmgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Нормалізація зображень\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Перетворення міток у one-hot вектори\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "train_images = np.array([np.resize(img, (32, 32, 3)) for img in train_images])\n",
        "test_images = np.array([np.resize(img, (32, 32, 3)) for img in test_images])\n",
        "\n",
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "vgg_base.trainable = False  # Заморожування ваг VGG16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    vgg_base,\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=10, validation_split=0.2)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('\\nТочність на тестових даних:', test_acc)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "test_labels_argmax = np.argmax(test_labels, axis=1)\n",
        "\n",
        "print(\"\\nМатриця помилок:\")\n",
        "print(confusion_matrix(test_labels_argmax, y_pred))\n",
        "\n",
        "print(\"\\nКласифікаційний звіт:\")\n",
        "print(classification_report(test_labels_argmax, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77x6Eez1Mqls",
        "outputId": "20e343d9-2ed7-4cdc-edc7-c7911d9e084b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 1.1109 - accuracy: 0.6114 - val_loss: 0.8442 - val_accuracy: 0.7138\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 17s 12ms/step - loss: 0.8772 - accuracy: 0.6989 - val_loss: 0.7665 - val_accuracy: 0.7370\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.8186 - accuracy: 0.7187 - val_loss: 0.7305 - val_accuracy: 0.7452\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.7913 - accuracy: 0.7276 - val_loss: 0.7098 - val_accuracy: 0.7533\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.7688 - accuracy: 0.7344 - val_loss: 0.7042 - val_accuracy: 0.7557\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.7518 - accuracy: 0.7401 - val_loss: 0.6769 - val_accuracy: 0.7605\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.7387 - accuracy: 0.7415 - val_loss: 0.6859 - val_accuracy: 0.7565\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.7282 - accuracy: 0.7464 - val_loss: 0.6647 - val_accuracy: 0.7654\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 17s 12ms/step - loss: 0.7197 - accuracy: 0.7492 - val_loss: 0.6600 - val_accuracy: 0.7678\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 17s 12ms/step - loss: 0.7108 - accuracy: 0.7512 - val_loss: 0.6556 - val_accuracy: 0.7723\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.6735 - accuracy: 0.7690\n",
            "\n",
            "Точність на тестових даних: 0.7689999938011169\n",
            "313/313 [==============================] - 2s 7ms/step\n",
            "\n",
            "Матриця помилок:\n",
            "[[703   5  64  87  21  21  83   0  13   3]\n",
            " [  6 902   6  69   3   3   5   0   3   3]\n",
            " [  9   3 701  13 160   5  88   0  16   5]\n",
            " [ 37  33  22 790  47  14  47   0   6   4]\n",
            " [  9   2 148  54 707  10  57   1   9   3]\n",
            " [  3   3   7   9   7 818  10 117  13  13]\n",
            " [151   4 170  63 170  12 389   0  34   7]\n",
            " [  0   0   0   0   0  54   0 898   3  45]\n",
            " [  3   1  19   5  23  15  10  15 899  10]\n",
            " [  6   0   5   5  17   9   3  59  13 883]]\n",
            "\n",
            "Класифікаційний звіт:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.70      0.73      1000\n",
            "           1       0.95      0.90      0.92      1000\n",
            "           2       0.61      0.70      0.65      1000\n",
            "           3       0.72      0.79      0.75      1000\n",
            "           4       0.61      0.71      0.66      1000\n",
            "           5       0.85      0.82      0.83      1000\n",
            "           6       0.56      0.39      0.46      1000\n",
            "           7       0.82      0.90      0.86      1000\n",
            "           8       0.89      0.90      0.89      1000\n",
            "           9       0.90      0.88      0.89      1000\n",
            "\n",
            "    accuracy                           0.77     10000\n",
            "   macro avg       0.77      0.77      0.77     10000\n",
            "weighted avg       0.77      0.77      0.77     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import cv2\n",
        "\n",
        "# Завантаження та підготовка датасету\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Зміна розміру зображень до 32x32 та конвертація у формат RGB\n",
        "train_images_rgb = np.array([cv2.cvtColor(cv2.resize(img, (32, 32)), cv2.COLOR_GRAY2RGB) for img in train_images]) / 255.0\n",
        "test_images_rgb = np.array([cv2.cvtColor(cv2.resize(img, (32, 32)), cv2.COLOR_GRAY2RGB) for img in test_images]) / 255.0\n",
        "\n",
        "# Перетворення міток у one-hot вектори\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "# Завантаження VGG16\n",
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# Розморожування останніх шарів VGG16\n",
        "for layer in vgg_base.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Створення кастомізованої моделі\n",
        "model = Sequential([\n",
        "    vgg_base,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Компіляція моделі\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Навчання моделі\n",
        "history = model.fit(train_images_rgb, train_labels, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Оцінка моделі\n",
        "test_loss, test_acc = model.evaluate(test_images_rgb, test_labels)\n",
        "print('\\nТочність на тестових даних:', test_acc)\n",
        "\n",
        "# Передбачення та отримання міток класів\n",
        "predictions = model.predict(test_images_rgb)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Матриця помилок\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(\"\\nМатриця помилок:\")\n",
        "print(confusion_matrix(true_classes, predicted_classes))\n",
        "\n",
        "# Класифікаційний звіт\n",
        "print(\"\\nКласифікаційний звіт:\")\n",
        "print(classification_report(true_classes, predicted_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymdHHXUASfeC",
        "outputId": "4e14d2ae-22aa-43cc-c374-74a759fbe690"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 29s 18ms/step - loss: 0.4962 - accuracy: 0.8255 - val_loss: 0.3332 - val_accuracy: 0.8804\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 29s 19ms/step - loss: 0.3209 - accuracy: 0.8871 - val_loss: 0.3156 - val_accuracy: 0.8832\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.2804 - accuracy: 0.8999 - val_loss: 0.2893 - val_accuracy: 0.8957\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 26s 18ms/step - loss: 0.2557 - accuracy: 0.9079 - val_loss: 0.2617 - val_accuracy: 0.9035\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 26s 18ms/step - loss: 0.2312 - accuracy: 0.9176 - val_loss: 0.2617 - val_accuracy: 0.9068\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 26s 18ms/step - loss: 0.2132 - accuracy: 0.9222 - val_loss: 0.2685 - val_accuracy: 0.9053\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 29s 19ms/step - loss: 0.1985 - accuracy: 0.9284 - val_loss: 0.2738 - val_accuracy: 0.9031\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 29s 19ms/step - loss: 0.1821 - accuracy: 0.9323 - val_loss: 0.2554 - val_accuracy: 0.9119\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 29s 19ms/step - loss: 0.1654 - accuracy: 0.9383 - val_loss: 0.2738 - val_accuracy: 0.9133\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 26s 18ms/step - loss: 0.1536 - accuracy: 0.9430 - val_loss: 0.2806 - val_accuracy: 0.9057\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 26s 18ms/step - loss: 0.1436 - accuracy: 0.9471 - val_loss: 0.3110 - val_accuracy: 0.9025\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.1329 - accuracy: 0.9521 - val_loss: 0.2943 - val_accuracy: 0.9069\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 29s 19ms/step - loss: 0.1213 - accuracy: 0.9552 - val_loss: 0.3374 - val_accuracy: 0.9078\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 29s 19ms/step - loss: 0.1115 - accuracy: 0.9585 - val_loss: 0.3239 - val_accuracy: 0.9071\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 29s 19ms/step - loss: 0.1045 - accuracy: 0.9624 - val_loss: 0.3279 - val_accuracy: 0.9142\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 26s 18ms/step - loss: 0.0933 - accuracy: 0.9655 - val_loss: 0.3526 - val_accuracy: 0.9147\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 29s 19ms/step - loss: 0.0931 - accuracy: 0.9655 - val_loss: 0.3671 - val_accuracy: 0.9122\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.0813 - accuracy: 0.9709 - val_loss: 0.3585 - val_accuracy: 0.9087\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 29s 19ms/step - loss: 0.0786 - accuracy: 0.9710 - val_loss: 0.3817 - val_accuracy: 0.9102\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 29s 19ms/step - loss: 0.0747 - accuracy: 0.9729 - val_loss: 0.3849 - val_accuracy: 0.9143\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.4411 - accuracy: 0.9061\n",
            "\n",
            "Точність на тестових даних: 0.9060999751091003\n",
            "313/313 [==============================] - 3s 7ms/step\n",
            "\n",
            "Матриця помилок:\n",
            "[[847   3  23  26   3   1  91   0   6   0]\n",
            " [  1 979   0  14   1   0   3   0   2   0]\n",
            " [ 11   1 866  11  67   0  44   0   0   0]\n",
            " [ 26   5  11 901  32   0  22   0   3   0]\n",
            " [  1   1  44  28 879   0  47   0   0   0]\n",
            " [  0   0   0   0   0 974   0  14   1  11]\n",
            " [119   0  65  26  83   0 697   0  10   0]\n",
            " [  0   0   0   0   0   9   0 963   1  27]\n",
            " [  3   0   0   4   3   2   2   0 986   0]\n",
            " [  0   0   1   0   0   3   0  27   0 969]]\n",
            "\n",
            "Класифікаційний звіт:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.84      1000\n",
            "           1       0.99      0.98      0.98      1000\n",
            "           2       0.86      0.87      0.86      1000\n",
            "           3       0.89      0.90      0.90      1000\n",
            "           4       0.82      0.88      0.85      1000\n",
            "           5       0.98      0.97      0.98      1000\n",
            "           6       0.77      0.70      0.73      1000\n",
            "           7       0.96      0.96      0.96      1000\n",
            "           8       0.98      0.99      0.98      1000\n",
            "           9       0.96      0.97      0.97      1000\n",
            "\n",
            "    accuracy                           0.91     10000\n",
            "   macro avg       0.91      0.91      0.91     10000\n",
            "weighted avg       0.91      0.91      0.91     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN показала значно кращі результати в порівнянні з багатошаровою мережею. Точність CNN була вищою і становила близько 91%, в той час як точність багатошарової мережі була нижчою і становила близько 87%. Це означає, що згорткова мережа ефективніше впоралася з класифікацією зображень Fashion MNIST.\n",
        "\n",
        "Висновок полягає в тому, що для обробки зображень, особливо для завдань класифікації зображень, CNN може бути кращим вибором порівняно з багатошаровими мережами. Використання згорткових шарів дозволяє моделі автоматично виявляти важливі ознаки на зображеннях і покращує її результати."
      ],
      "metadata": {
        "id": "1XUWxcZefko5"
      }
    }
  ]
}