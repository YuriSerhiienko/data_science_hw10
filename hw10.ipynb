{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKOn2py5B/ZmpJlhiIOvYJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuriSerhiienko/data_science_hw10/blob/main/hw10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D"
      ],
      "metadata": {
        "id": "x-qeLdCYiwJb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo41pF7drehj",
        "outputId": "76882423-3603-4588-d5d0-91addc50cf2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 9s 5ms/step - loss: 0.4733 - accuracy: 0.8285 - val_loss: 0.3581 - val_accuracy: 0.8702\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3163 - accuracy: 0.8858 - val_loss: 0.3229 - val_accuracy: 0.8827\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2699 - accuracy: 0.9004 - val_loss: 0.2833 - val_accuracy: 0.8954\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2348 - accuracy: 0.9135 - val_loss: 0.2550 - val_accuracy: 0.9048\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2121 - accuracy: 0.9210 - val_loss: 0.2574 - val_accuracy: 0.9075\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1893 - accuracy: 0.9301 - val_loss: 0.2638 - val_accuracy: 0.9098\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1671 - accuracy: 0.9383 - val_loss: 0.2655 - val_accuracy: 0.9039\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1475 - accuracy: 0.9444 - val_loss: 0.2701 - val_accuracy: 0.9080\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1320 - accuracy: 0.9509 - val_loss: 0.2869 - val_accuracy: 0.9031\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1185 - accuracy: 0.9561 - val_loss: 0.2794 - val_accuracy: 0.9118\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3018 - accuracy: 0.9096\n",
            "\n",
            "Точність на тестових даних: 0.909600019454956\n",
            "313/313 [==============================] - 1s 3ms/step\n",
            "\n",
            "Матриця помилок:\n",
            "[[854   0  20  13   1   1 108   0   3   0]\n",
            " [  3 977   0  10   3   0   5   0   2   0]\n",
            " [ 22   1 843   8  44   0  82   0   0   0]\n",
            " [ 23   4   7 911  19   0  34   0   2   0]\n",
            " [  0   0  46  28 846   1  76   0   3   0]\n",
            " [  0   0   0   0   0 987   0  11   0   2]\n",
            " [104   0  44  21  46   0 781   0   4   0]\n",
            " [  0   0   0   0   0  10   0 972   0  18]\n",
            " [  5   0   4   2   3   2  10   3 971   0]\n",
            " [  1   0   0   0   0   6   0  39   0 954]]\n",
            "\n",
            "Класифікаційний звіт:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.85      1000\n",
            "           1       0.99      0.98      0.99      1000\n",
            "           2       0.87      0.84      0.86      1000\n",
            "           3       0.92      0.91      0.91      1000\n",
            "           4       0.88      0.85      0.86      1000\n",
            "           5       0.98      0.99      0.98      1000\n",
            "           6       0.71      0.78      0.75      1000\n",
            "           7       0.95      0.97      0.96      1000\n",
            "           8       0.99      0.97      0.98      1000\n",
            "           9       0.98      0.95      0.97      1000\n",
            "\n",
            "    accuracy                           0.91     10000\n",
            "   macro avg       0.91      0.91      0.91     10000\n",
            "weighted avg       0.91      0.91      0.91     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Нормалізація зображень\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Зміна розмірів зображень для CNN\n",
        "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
        "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=10, validation_split=0.2)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('\\nТочність на тестових даних:', test_acc)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"\\nМатриця помилок:\")\n",
        "print(confusion_matrix(test_labels, y_pred))\n",
        "\n",
        "print(\"\\nКласифікаційний звіт:\")\n",
        "print(classification_report(test_labels, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Завантаження та підготовка датасету\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Зміна розмірів зображень для VGG16\n",
        "def resize_images(images):\n",
        "    images_resized = np.array([tf.image.resize(img, (32, 32)) for img in images])\n",
        "    return images_resized\n",
        "\n",
        "train_images_resized = resize_images(train_images[..., np.newaxis])\n",
        "test_images_resized = resize_images(test_images[..., np.newaxis])\n",
        "\n",
        "# Конвертація зображень у формат RGB\n",
        "train_images_rgb = np.repeat(train_images_resized, 3, axis=-1)\n",
        "test_images_rgb = np.repeat(test_images_resized, 3, axis=-1)\n",
        "\n",
        "# Завантаження VGG16\n",
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "vgg_base.trainable = False  # Заморожування ваг\n",
        "\n",
        "# Створення моделі\n",
        "model = tf.keras.Sequential([\n",
        "    vgg_base,\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Навчання моделі\n",
        "history = model.fit(train_images_rgb, train_labels, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Оцінка моделі\n",
        "test_loss, test_acc = model.evaluate(test_images_rgb, test_labels)\n",
        "print('\\nТочність на тестових даних:', test_acc)\n",
        "\n",
        "# Передбачення та метрики якості\n",
        "predictions = model.predict(test_images_rgb)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"\\nМатриця помилок:\")\n",
        "print(confusion_matrix(test_labels, y_pred))\n",
        "\n",
        "print(\"\\nКласифікаційний звіт:\")\n",
        "print(classification_report(test_labels, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvFE4hPPvPLS",
        "outputId": "4d2f7e28-e649-470a-cda7-d4f7f3063e56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.6873 - accuracy: 0.7614 - val_loss: 0.4779 - val_accuracy: 0.8272\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.5048 - accuracy: 0.8208 - val_loss: 0.4380 - val_accuracy: 0.8355\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.4679 - accuracy: 0.8307 - val_loss: 0.4253 - val_accuracy: 0.8438\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4441 - accuracy: 0.8383 - val_loss: 0.4018 - val_accuracy: 0.8513\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.4279 - accuracy: 0.8443 - val_loss: 0.4049 - val_accuracy: 0.8505\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.4169 - accuracy: 0.8453 - val_loss: 0.3918 - val_accuracy: 0.8539\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.4092 - accuracy: 0.8495 - val_loss: 0.3932 - val_accuracy: 0.8507\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.4006 - accuracy: 0.8530 - val_loss: 0.3810 - val_accuracy: 0.8606\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.3912 - accuracy: 0.8556 - val_loss: 0.3826 - val_accuracy: 0.8594\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.3873 - accuracy: 0.8577 - val_loss: 0.3706 - val_accuracy: 0.8633\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.3729 - accuracy: 0.8634\n",
            "\n",
            "Точність на тестових даних: 0.8633999824523926\n",
            "313/313 [==============================] - 2s 7ms/step\n",
            "\n",
            "Матриця помилок:\n",
            "[[828   0  15  47   8   1  89   1  11   0]\n",
            " [  0 960   4  31   1   0   3   0   1   0]\n",
            " [ 12   0 761  14 106   0 102   0   5   0]\n",
            " [ 28   9  13 856  47   0  44   0   3   0]\n",
            " [  6   2  73  43 793   0  82   0   1   0]\n",
            " [  0   0   0   0   0 947   0  41   1  11]\n",
            " [139   2  62  45 116   0 622   0  14   0]\n",
            " [  0   0   0   0   0  26   0 951   0  23]\n",
            " [  6   1   3   7   6   5   3   1 967   1]\n",
            " [  0   0   1   0   0  10   0  40   0 949]]\n",
            "\n",
            "Класифікаційний звіт:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82      1000\n",
            "           1       0.99      0.96      0.97      1000\n",
            "           2       0.82      0.76      0.79      1000\n",
            "           3       0.82      0.86      0.84      1000\n",
            "           4       0.74      0.79      0.76      1000\n",
            "           5       0.96      0.95      0.95      1000\n",
            "           6       0.66      0.62      0.64      1000\n",
            "           7       0.92      0.95      0.94      1000\n",
            "           8       0.96      0.97      0.97      1000\n",
            "           9       0.96      0.95      0.96      1000\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.86      0.86      0.86     10000\n",
            "weighted avg       0.86      0.86      0.86     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попередня робота (Багатошаровий перцептрон)\n",
        "\n",
        "Архітектура: Базувалася на повнозв'язних шарах (Dense layers).\n",
        "\n",
        "Точність на тестових даних: 88.08% у базовій моделі, 86.93% після оновлення архітектури.\n",
        "\n",
        "Інші метрики: Висока точність (precision) та повнота (recall), особливо після оновлення архітектури.\n",
        "\n",
        "Нинішня робота (Згорткова нейронна мережа, CNN)\n",
        "\n",
        "Архітектура: Базувалася на згорткових шарах (Conv2D), що є типовим для задач обробки зображень.\n",
        "\n",
        "Точність на тестових даних: 90.88%.\n",
        "\n",
        "Інші метрики: Деталізовані метрики (матриця помилок, точність, повнота, F1-оцінка) показали високу ефективність моделі у класифікації зображень.\n",
        "\n",
        "Висновки\n",
        "\n",
        "Ефективність архітектури: Згорткова нейронна мережа (CNN) показала кращі результати для задачі класифікації зображень у порівнянні з багатошаровим перцептроном. Це очікувано, оскільки CNN краще адаптовані для виявлення шаблонів у зображеннях.\n",
        "\n",
        "Підхід до обробки даних: CNN використовує просторові відносини між пікселями, що дозволяє ефективно виділяти особливості зображень, в той час як багатошаровий перцептрон обробляє дані як одновимірний масив.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZxPkg8zZMmgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Нормалізація зображень\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Перетворення міток у one-hot вектори\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "train_images = np.array([np.resize(img, (32, 32, 3)) for img in train_images])\n",
        "test_images = np.array([np.resize(img, (32, 32, 3)) for img in test_images])\n",
        "\n",
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "vgg_base.trainable = False  # Заморожування ваг VGG16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    vgg_base,\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=10, validation_split=0.2)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('\\nТочність на тестових даних:', test_acc)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "test_labels_argmax = np.argmax(test_labels, axis=1)\n",
        "\n",
        "print(\"\\nМатриця помилок:\")\n",
        "print(confusion_matrix(test_labels_argmax, y_pred))\n",
        "\n",
        "print(\"\\nКласифікаційний звіт:\")\n",
        "print(classification_report(test_labels_argmax, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77x6Eez1Mqls",
        "outputId": "cff76a27-4f3c-4e5a-e697-204d4a86a3a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 1.1173 - accuracy: 0.6127 - val_loss: 0.8467 - val_accuracy: 0.7089\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.8862 - accuracy: 0.6960 - val_loss: 0.7723 - val_accuracy: 0.7339\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.8249 - accuracy: 0.7168 - val_loss: 0.7343 - val_accuracy: 0.7442\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.7943 - accuracy: 0.7263 - val_loss: 0.7319 - val_accuracy: 0.7462\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.7696 - accuracy: 0.7346 - val_loss: 0.6986 - val_accuracy: 0.7552\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 17s 12ms/step - loss: 0.7530 - accuracy: 0.7380 - val_loss: 0.6845 - val_accuracy: 0.7622\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.7420 - accuracy: 0.7438 - val_loss: 0.6729 - val_accuracy: 0.7657\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.7341 - accuracy: 0.7445 - val_loss: 0.6764 - val_accuracy: 0.7657\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.7217 - accuracy: 0.7476 - val_loss: 0.6664 - val_accuracy: 0.7677\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.7183 - accuracy: 0.7496 - val_loss: 0.6790 - val_accuracy: 0.7599\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.6964 - accuracy: 0.7562\n",
            "\n",
            "Точність на тестових даних: 0.7562000155448914\n",
            "313/313 [==============================] - 2s 7ms/step\n",
            "\n",
            "Матриця помилок:\n",
            "[[788   1  37  87   4  15  56   0  11   1]\n",
            " [ 13 876   6  93   2   1   4   0   3   2]\n",
            " [ 27   4 719  29  53   7 144   0  14   3]\n",
            " [ 62  16  12 840  11   9  41   0   5   4]\n",
            " [ 23   0 223  84 486  13 150   1  12   8]\n",
            " [ 11   1   9  15   1 828   9 102  12  12]\n",
            " [223   4 187  80  61  11 391   0  38   5]\n",
            " [  0   0   0   0   0  67   0 855   2  76]\n",
            " [  8   1  22   5   6  26  13  15 886  18]\n",
            " [ 21   0   3   9   6  15   5  44   4 893]]\n",
            "\n",
            "Класифікаційний звіт:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.79      0.72      1000\n",
            "           1       0.97      0.88      0.92      1000\n",
            "           2       0.59      0.72      0.65      1000\n",
            "           3       0.68      0.84      0.75      1000\n",
            "           4       0.77      0.49      0.60      1000\n",
            "           5       0.83      0.83      0.83      1000\n",
            "           6       0.48      0.39      0.43      1000\n",
            "           7       0.84      0.85      0.85      1000\n",
            "           8       0.90      0.89      0.89      1000\n",
            "           9       0.87      0.89      0.88      1000\n",
            "\n",
            "    accuracy                           0.76     10000\n",
            "   macro avg       0.76      0.76      0.75     10000\n",
            "weighted avg       0.76      0.76      0.75     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Завантаження та підготовка датасету\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Зміна розміру зображень до 32x32 та конвертація у формат RGB\n",
        "train_images_rgb = np.array([cv2.cvtColor(cv2.resize(img, (32, 32)), cv2.COLOR_GRAY2RGB) for img in train_images]) / 255.0\n",
        "test_images_rgb = np.array([cv2.cvtColor(cv2.resize(img, (32, 32)), cv2.COLOR_GRAY2RGB) for img in test_images]) / 255.0\n",
        "\n",
        "# Перетворення міток у one-hot вектори\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "# Завантаження VGG16\n",
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# Розморожування останніх шарів VGG16\n",
        "for layer in vgg_base.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Створення кастомізованої моделі\n",
        "model = Sequential([\n",
        "    vgg_base,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Компіляція моделі\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Навчання моделі\n",
        "history = model.fit(train_images_rgb, train_labels, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Оцінка моделі\n",
        "test_loss, test_acc = model.evaluate(test_images_rgb, test_labels)\n",
        "print('\\nТочність на тестових даних:', test_acc)\n",
        "\n",
        "# Передбачення та отримання міток класів\n",
        "predictions = model.predict(test_images_rgb)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Матриця помилок\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(\"\\nМатриця помилок:\")\n",
        "print(confusion_matrix(true_classes, predicted_classes))\n",
        "\n",
        "# Класифікаційний звіт\n",
        "print(\"\\nКласифікаційний звіт:\")\n",
        "print(classification_report(true_classes, predicted_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymdHHXUASfeC",
        "outputId": "dcb025ad-7c9f-4633-d814-46d77fe381d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 28s 17ms/step - loss: 0.4968 - accuracy: 0.8253 - val_loss: 0.3554 - val_accuracy: 0.8725\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.3229 - accuracy: 0.8868 - val_loss: 0.3089 - val_accuracy: 0.8898\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.2801 - accuracy: 0.9003 - val_loss: 0.2894 - val_accuracy: 0.8962\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.2540 - accuracy: 0.9077 - val_loss: 0.2917 - val_accuracy: 0.8920\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.2336 - accuracy: 0.9151 - val_loss: 0.2654 - val_accuracy: 0.9082\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 0.2107 - accuracy: 0.9246 - val_loss: 0.2746 - val_accuracy: 0.9050\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 0.1986 - accuracy: 0.9293 - val_loss: 0.2592 - val_accuracy: 0.9091\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 0.1786 - accuracy: 0.9346 - val_loss: 0.2789 - val_accuracy: 0.9085\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 0.1665 - accuracy: 0.9405 - val_loss: 0.2770 - val_accuracy: 0.9094\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 0.1529 - accuracy: 0.9441 - val_loss: 0.2794 - val_accuracy: 0.9105\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.1407 - accuracy: 0.9476 - val_loss: 0.3067 - val_accuracy: 0.9100\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.1314 - accuracy: 0.9517 - val_loss: 0.3065 - val_accuracy: 0.9098\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.1246 - accuracy: 0.9541 - val_loss: 0.2970 - val_accuracy: 0.9126\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.1106 - accuracy: 0.9593 - val_loss: 0.3231 - val_accuracy: 0.9115\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 0.1011 - accuracy: 0.9632 - val_loss: 0.3367 - val_accuracy: 0.9066\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 0.0959 - accuracy: 0.9651 - val_loss: 0.3392 - val_accuracy: 0.9122\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 0.0882 - accuracy: 0.9672 - val_loss: 0.4188 - val_accuracy: 0.9109\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.0821 - accuracy: 0.9689 - val_loss: 0.4133 - val_accuracy: 0.9052\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.0748 - accuracy: 0.9714 - val_loss: 0.4254 - val_accuracy: 0.9125\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.0738 - accuracy: 0.9727 - val_loss: 0.4119 - val_accuracy: 0.9143\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.4338 - accuracy: 0.9052\n",
            "\n",
            "Точність на тестових даних: 0.9052000045776367\n",
            "313/313 [==============================] - 2s 7ms/step\n",
            "\n",
            "Матриця помилок:\n",
            "[[825   0  26  32   1   0 101   0  15   0]\n",
            " [  1 970   3  20   1   0   4   0   1   0]\n",
            " [ 14   0 887  10  41   0  47   0   1   0]\n",
            " [ 21   1  14 899  34   0  25   0   6   0]\n",
            " [  0   0  79  29 844   0  46   0   2   0]\n",
            " [  0   0   0   0   0 986   0  12   1   1]\n",
            " [ 89   0  77  33  79   0 714   0   8   0]\n",
            " [  0   0   0   0   0  12   0 970   0  18]\n",
            " [  1   0   3   0   0   0   1   0 995   0]\n",
            " [  0   0   0   0   0   5   0  32   1 962]]\n",
            "\n",
            "Класифікаційний звіт:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.82      0.85      1000\n",
            "           1       1.00      0.97      0.98      1000\n",
            "           2       0.81      0.89      0.85      1000\n",
            "           3       0.88      0.90      0.89      1000\n",
            "           4       0.84      0.84      0.84      1000\n",
            "           5       0.98      0.99      0.98      1000\n",
            "           6       0.76      0.71      0.74      1000\n",
            "           7       0.96      0.97      0.96      1000\n",
            "           8       0.97      0.99      0.98      1000\n",
            "           9       0.98      0.96      0.97      1000\n",
            "\n",
            "    accuracy                           0.91     10000\n",
            "   macro avg       0.91      0.91      0.90     10000\n",
            "weighted avg       0.91      0.91      0.90     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN показала значно кращі результати в порівнянні з багатошаровою мережею. Точність CNN була вищою і становила близько 91%, в той час як точність багатошарової мережі була нижчою і становила близько 87%. Це означає, що згорткова мережа ефективніше впоралася з класифікацією зображень Fashion MNIST.\n",
        "\n",
        "Висновок полягає в тому, що для обробки зображень, особливо для завдань класифікації зображень, CNN може бути кращим вибором порівняно з багатошаровими мережами. Використання згорткових шарів дозволяє моделі автоматично виявляти важливі ознаки на зображеннях і покращує її результати."
      ],
      "metadata": {
        "id": "1XUWxcZefko5"
      }
    }
  ]
}